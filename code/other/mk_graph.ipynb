{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx, graphviz\n",
    "import pandas as pd, numpy as np,xarray as xr, plotly\n",
    "from pathlib import Path\n",
    "import re, yaml, copy, json\n",
    "import helper, config_adapter\n",
    "from helper import RenderJSON\n",
    "from copy import deepcopy\n",
    "plotly.offline.init_notebook_mode()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itables\n",
    "itables.init_notebook_mode(all_interactive=True )\n",
    "itables.options.maxBytes = \"1MB\"\n",
    "itables.options.lengthMenu = [25, 10, 50, 100, 200]\n",
    "itables.options.buttons = [\"copyHtml5\", \"csvHtml5\", \"excelHtml5\"]\n",
    "itables.options.layout={\"topEnd\": \"pageLength\", \"top1\": \"searchBuilder\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = yaml.safe_load(Path(\"params.yaml\").open(\"r\"))\n",
    "# params = dict(\n",
    "#     variables=dict(\n",
    "#         task_file='/home/julienb/Documents/Data/Raphael/Poly_Exercices/LASER_Task7_v3_100LeftLever_LeftHanded_PADComp_Check-PADS@RT_ITI400-1200_RT2000_MT3500_Errors_6secOFF_Partial300MT_Laser3070_ContiL1only_Nico_GOOD.xls',\n",
    "#         dat_file='/home/julienb/Documents/Data/Raphael/Poly_Data/#517/01072024/Rat_#517_Ambidexter_LeftHemiStimCTRL_Beta300MT_Laser3070_L1L25050_01072024_01.dat',\n",
    "#     ),\n",
    "#     config_path='/home/julienb/Documents/database_scripts/templates/mk_graph.yaml'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = Path(params[\"config_path\"])\n",
    "if \"variables\" in params:\n",
    "    variables = config_adapter.normalize_yaml_paramlist(params[\"variables\"], format=config_adapter.variable_param_format)\n",
    "else: \n",
    "    variables = []\n",
    "RenderJSON(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = config_adapter.load(config_path)\n",
    "RenderJSON(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"variables\" in config:\n",
    "    variables += config_adapter.normalize_yaml_paramlist(config[\"variables\"], format=config_adapter.variable_param_format)\n",
    "display(RenderJSON(variables))\n",
    "ctx = config_adapter.Context()\n",
    "for var in variables:\n",
    "    config_adapter.add_variable_context(ctx, var)\n",
    "RenderJSON(ctx.variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_poly_task(ctx, params):\n",
    "    params = ctx.evaluate(params)\n",
    "    task_path = params[\"task_file\"]\n",
    "    with Path(task_path).open(\"r\") as f:\n",
    "        i=0\n",
    "        while(True):\n",
    "            l = f.readline().split(\"\\t\")\n",
    "            if len([x for x in l if \"NEXT\" in x]) >1:\n",
    "                break\n",
    "            i+=1\n",
    "    task_df = pd.read_csv(task_path, sep=\"\\t\", skiprows=i)\n",
    "    # header_line = task_df.str.contains(\"NEXT\").astype(int).sum(axis=1).argmax()\n",
    "    # task_df.columns = task_df.iloc[header_line, :]\n",
    "    # task_df = task_df.iloc[header_line+1:, :]\n",
    "    task_df = task_df.rename(columns={task_df.columns[0]: \"task_node\" })\n",
    "    df = task_df\n",
    "    df = df.loc[~pd.isna(df[\"task_node\"])]\n",
    "    df = df.dropna(subset=df.columns[1:], how=\"all\")\n",
    "    df[\"task_node\"] = df[\"task_node\"].astype(int)\n",
    "    graph = nx.DiGraph()\n",
    "    for _, row in df.iterrows():\n",
    "        row = row.dropna().to_dict()\n",
    "        names = []\n",
    "        graph.add_node(row[\"task_node\"])\n",
    "        node = graph.nodes[row[\"task_node\"]]\n",
    "        for col in row:\n",
    "            if col.startswith(\"NEXT\"):\n",
    "                pattern = r'\\(.+\\)$'\n",
    "                ns = re.findall(pattern, row[col])\n",
    "                if len(ns) == 0:\n",
    "                    next_line = row[\"task_node\"]+1\n",
    "                    cond = row[col]\n",
    "                elif len(ns) ==1:\n",
    "                    cond = row[col][:-len(ns[0])]\n",
    "                    nlname = ns[0][1: -1]\n",
    "                    if re.match(r'\\d+', nlname):\n",
    "                        next_line = int(nlname)\n",
    "                    else:\n",
    "                        next_line = df.loc[(df[[\"T1\", \"T2\", \"T3\"]].apply(lambda s: s.str.lstrip(\"_\")) == nlname).any(axis=1)][\"task_node\"]\n",
    "                        if len(next_line) != 1:\n",
    "                            raise Exception(f\"problem {len(next_line)} {nlname}\")\n",
    "                        next_line = next_line.iat[0]\n",
    "                else:\n",
    "                    raise Exception(\"Problem\")\n",
    "                graph.add_edge(row[\"task_node\"], next_line, cond=cond)\n",
    "            elif re.match(\"T\\d+\", col):\n",
    "                m = re.match(r'(?P<time>\\d*-?\\d*)_(?P<name>\\w+)$', str(row[col]))\n",
    "                if not m is None:\n",
    "                    names.append(m[\"name\"])\n",
    "                    if m[\"time\"]:\n",
    "                        node[col] = m[\"time\"]\n",
    "                else:\n",
    "                    node[col] = row[col]\n",
    "            else:\n",
    "                node[col] = row[col]\n",
    "        node[\"poly_names\"] = names\n",
    "    return graph\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_poly_dat(ctx, params):\n",
    "    params = ctx.evaluate(params)\n",
    "    dat_path = params[\"dat_file\"]\n",
    "    event_df = pd.read_csv(dat_path, sep=\"\\t\", names=['time (ms)', 'family', 'nbre', '_P', '_V', '_L', '_R', '_T', '_W', '_X', '_Y', '_Z'], skiprows=13, dtype=int)\n",
    "    event_df.insert(0, \"t\", event_df.pop(\"time (ms)\")/1000)\n",
    "    event_df.insert(1, \"next_t\", event_df[\"t\"].shift(-1))\n",
    "    event_df[\"task_node\"] = event_df[\"_T\"].where(event_df[\"family\"]==10).ffill()\n",
    "    event_df[\"next_node\"] = event_df[\"_T\"].where(event_df[\"family\"]==10).shift(-1).bfill()\n",
    "    event_df[\"node_change\"] = (event_df[\"family\"]==10).cumsum()\n",
    "    grp = event_df.groupby(\"node_change\")\n",
    "    graph = nx.DiGraph()\n",
    "    if \"node_info\" in params:\n",
    "        final = pd.DataFrame()\n",
    "        for n in params[\"node_info\"]:\n",
    "            grp_value = grp.apply(\n",
    "                lambda d: pd.Series(dict(task_node=d[\"task_node\"].iat[0], group=d.eval(str(n[\"group_expr\"])))), include_groups=False).reset_index(drop=True)\n",
    "            res = grp_value.groupby(\"task_node\").apply(lambda d: d.eval(str(n[\"agg_expr\"])), include_groups=False)\n",
    "            final[n[\"name\"]] = res\n",
    "        for n, row in final.iterrows():\n",
    "            graph.add_node(n, **row.to_dict())\n",
    "    if \"edge_info\" in params:\n",
    "        final = pd.DataFrame()\n",
    "        for n in params[\"edge_info\"]:\n",
    "            grp_value = grp.apply(\n",
    "                lambda d: pd.Series(dict(task_node=d[\"task_node\"].iat[0], next_node=d[\"next_node\"].iat[0], group=d.eval(str(n[\"group_expr\"])))), include_groups=False).reset_index(drop=True)\n",
    "            res = grp_value.groupby([\"task_node\", \"next_node\"]).apply(lambda d: d.eval(str(n[\"agg_expr\"])), include_groups=False)\n",
    "            final[n[\"name\"]] = res\n",
    "        for (n1, n2), row in final.iterrows():\n",
    "            graph.add_edge(n1, n2, **row.to_dict())\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx.methods[\"from_poly_task\"] = from_poly_task\n",
    "ctx.methods[\"from_poly_dat\"] = from_poly_dat\n",
    "combined_graph = nx.DiGraph()\n",
    "for g in config[\"processing\"][\"graphs\"]:\n",
    "    graph = ctx.evaluate(g)\n",
    "    node_infos_df = pd.DataFrame([dict(node=n) | v for n, v in graph.nodes(data=True)])\n",
    "    edge_infos_df = pd.DataFrame([dict(src=n1, dest=n2) | v  for n1, n2,v in graph.edges(data=True)])\n",
    "    display(node_infos_df)\n",
    "    display(edge_infos_df)\n",
    "    combined_graph = nx.compose(combined_graph, graph)\n",
    "del ctx.methods[\"from_poly_task\"]\n",
    "del ctx.methods[\"from_poly_dat\"]\n",
    "json_graph = json.dumps(nx.cytoscape_data(graph), indent=4)\n",
    "with Path(\"graph.json\").open(\"w\") as f:\n",
    "    f.write(json_graph)\n",
    "RenderJSON(json_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_graph = deepcopy(combined_graph)\n",
    "for node, attrs in display_graph.nodes(data=True):\n",
    "    label = '\\n'.join([f'{k}: {v}' for k,v in attrs.items()])\n",
    "    for attr in list(attrs):\n",
    "        del attrs[attr]\n",
    "    display_graph.nodes[node][\"label\"] = label\n",
    "\n",
    "for n1, n2, attrs in display_graph.edges(data=True):\n",
    "    label = '\\n'.join([f'{k}: {v}' for k,v in attrs.items()])\n",
    "    for attr in list(attrs):\n",
    "        del attrs[attr]\n",
    "    display_graph.edges[n1, n2][\"label\"] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.nx_pydot.write_dot(display_graph, \"graph.dot\")\n",
    "graphviz.render(\"dot\", filepath=\"graph.dot\", outfile=\"graph.svg\", format=\"svg\")\n",
    "from IPython.display import SVG\n",
    "SVG(filename=\"graph.svg\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dbscripts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
