{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np,xarray as xr\n",
    "from pathlib import Path\n",
    "import re, yaml, copy, json\n",
    "import helper, events_methods\n",
    "import subprocess\n",
    "import plotly\n",
    "from helper import RenderJSON\n",
    "plotly.offline.init_notebook_mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itables\n",
    "itables.init_notebook_mode(all_interactive=True )\n",
    "itables.options.maxBytes = \"1MB\"\n",
    "itables.options.lengthMenu = [25, 10, 50, 100, 200]\n",
    "itables.options.buttons = [\"copyHtml5\", \"csvHtml5\", \"excelHtml5\"]\n",
    "# itables.options.scrollY=\"200px\"\n",
    "# itables.options.scrollCollapse=True\n",
    "# itables.options.paging=False\n",
    "# itables.options.column_filters = \"footer\"\n",
    "itables.options.layout={\"topEnd\": \"pageLength\", \"top1\": \"searchBuilder\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = yaml.safe_load(Path(\"params.yaml\").open(\"r\"))\n",
    "spike2_path = Path(params[\"smrx_path\"])\n",
    "info_path = Path(params[\"config_path\"])\n",
    "res_events_path = Path(params[\"dest_path\"])\n",
    "RenderJSON(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not spike2_path.with_suffix(\"\").with_stem(spike2_path.stem+ \"_data\").exists():\n",
    "    subprocess.run([\"/home/julienb/miniconda3/envs/spike2/bin/smrx2python\", \"-i\", str(spike2_path)])\n",
    "channels = pd.read_csv(spike2_path.with_suffix(\".tsv\"), sep=\"\\t\")\n",
    "channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = yaml.safe_load(info_path.open(\"r\"))\n",
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discretize = events_methods.EventProcessing.process_info(channels[\"name\"].to_list(), info[\"discretize\"], dest_name=\"dest_channel\")\n",
    "chans = set(channels[\"name\"].to_list())\n",
    "regular_chans =  set(channels[\"name\"].loc[channels[\"data_kind\"] == \"RegularSampling\"].to_list())\n",
    "dkeys = {v[\"channel\"] for v in discretize.values()}\n",
    "if not dkeys.issubset(chans):\n",
    "    raise Exception(f\"Some source channels where not found {dkeys - chans}\")\n",
    "if not dkeys.issubset(regular_chans):\n",
    "    raise Exception(f\"Some source channels are not regular (continuous) {dkeys - regular_chans}\")\n",
    "pd.DataFrame(list(discretize.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discretized = []\n",
    "chans=[]\n",
    "for dest_chan, item in discretize.items():\n",
    "    npy = np.load(spike2_path.with_suffix(\"\").with_stem(spike2_path.stem+ \"_data\") /( item[\"channel\"] + \".npy\"))\n",
    "    if len(channels[channels[\"name\"] == item[\"channel\"]].index) != 1:\n",
    "        raise Exception(\"Problem\")\n",
    "    meta = channels[channels[\"name\"] == item[\"channel\"]].iloc[0, :]\n",
    "    data = xr.DataArray(npy, dims=[\"t\"])\n",
    "    data[\"t\"] = np.arange(npy.size)/meta[\"fs\"]\n",
    "    events = events_methods.Discretize.call(item[\"method\"],data, item)\n",
    "    n_interp_start=10**7\n",
    "    n_interp_val = 10**6\n",
    "    if data.size > n_interp_start:\n",
    "        display_data = data.interp(t=np.linspace(data[\"t\"].min().item(),data[\"t\"].max().item(), n_interp_val))\n",
    "        display_name = item[\"channel\"] + \"_interp\"\n",
    "    else:\n",
    "        display_data = data\n",
    "        display_name = item[\"channel\"]\n",
    "    # display(data)\n",
    "    import plotly.graph_objects as go\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=display_data[\"t\"].to_numpy(), y=display_data.to_numpy(), name=display_name))\n",
    "    ys = [data.min().item(), data.max().item()]\n",
    "    for _, row in events.iterrows():\n",
    "        fig.add_trace(go.Scatter(x=[row[\"t\"]]*2, y=ys, line_color=\"green\" if row[\"State\"] else \"red\", showlegend=False))\n",
    "        # fig.add_vline(x=row[\"t\"], line_color=\"green\" if row[\"State\"] else \"red\")\n",
    "    fig.show()\n",
    "    # display(to_display)\n",
    "    discretized.append(events)\n",
    "    chans.append(dest_chan)\n",
    "    # display(events)\n",
    "    \n",
    "discretized = pd.concat(discretized).sort_values(\"t\").reset_index(drop=True)\n",
    "discretized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_df = [discretized]\n",
    "for _, row in channels.loc[channels[\"data_kind\"]==\"Event\"].iterrows():\n",
    "    data = np.load(spike2_path.with_suffix(\"\").with_stem(spike2_path.stem+ \"_data\") /(row[\"name\"] + \".npy\"))\n",
    "    chans.append(row[\"name\"])\n",
    "    if row[\"smrx_type\"] == \"DataType.EventBoth\":\n",
    "        dr = pd.DataFrame().assign(t=data[:, 0], channel_name=row[\"name\"], State=1)\n",
    "        dd = pd.DataFrame().assign(t=data[: , 1], channel_name=row[\"name\"], State=0)\n",
    "        d = pd.concat([dd, dr])\n",
    "    else: raise Exception(\"unhandled event type\")\n",
    "    event_df.append(d)\n",
    "event_df = pd.concat(event_df).sort_values(\"t\").reset_index(drop=True)\n",
    "event_df\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_spec = events_methods.EventProcessing.process_info(chans, info[\"processing\"])\n",
    "pd.DataFrame(list(event_spec.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all=[]\n",
    "for ev_name, item in event_spec.items():\n",
    "    ev_dataframe = events_methods.FiberEventProcessing.compute_evdataframe(event_df, item)\n",
    "    if len(ev_dataframe.index) == 0: continue\n",
    "    events = events_methods.FiberEventProcessing.call(item[\"method\"],ev_dataframe, item)\n",
    "    if len(events.index)!=0:\n",
    "        all.append(events)\n",
    "all = pd.concat(all).sort_values(\"t\")\n",
    "all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"display\" in info and \"rename\" in info[\"display\"]:\n",
    "    all[\"event_name\"] = all[\"event_name\"].map(lambda e: info[\"display\"][\"rename\"][e] if e in info[\"display\"][\"rename\"] else e)\n",
    "json_cols = [\"metadata\", \"waveform_changes\", \"waveform_values\"]\n",
    "for col in json_cols:\n",
    "    all[f\"{col}_json\"] = all[col].apply(lambda d: json.dumps(d))\n",
    "all.drop(columns=json_cols).to_csv(res_events_path, sep=\"\\t\", index=False)\n",
    "reloaded = pd.read_csv(res_events_path, sep=\"\\t\", index_col=False)\n",
    "for col in reloaded.columns:\n",
    "    if col.endswith(\"_json\"):\n",
    "        reloaded[col[:-5]] = reloaded.pop(col).apply(lambda s: json.loads(s) if not pd.isna(s) else None)\n",
    "reloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = events_methods.EventProcessing.summarize(reloaded)\n",
    "summary\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dbscripts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
