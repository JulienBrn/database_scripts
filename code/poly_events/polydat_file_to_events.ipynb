{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np,xarray as xr\n",
    "from pathlib import Path\n",
    "import re, yaml, copy, json\n",
    "from helper import singleglob, json_merge\n",
    "import events_methods\n",
    "import itables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "itables.init_notebook_mode(all_interactive=True )\n",
    "itables.options.maxBytes = \"1MB\"\n",
    "itables.options.lengthMenu = [25, 10, 50, 100, 200]\n",
    "# itables.options.scrollY=\"200px\"\n",
    "# itables.options.scrollCollapse=True\n",
    "# itables.options.paging=False\n",
    "itables.options.column_filters = \"footer\"\n",
    "itables.options.layout={\"topEnd\": None, \"top1\": \"searchBuilder\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base = Path(\"/home/julienb/Documents/database_scripts/database_scripts_test/poly_dat_files/Rats/Test_Julien_ForcedInput/\")\n",
    "base = Path(\"/home/julienb/Documents/database_scripts/database_scripts_test/poly_dat_files/Rats/Luisa/Rat101_0729_opto_01\")\n",
    "# base = Path(\"/home/julienb/Documents/database_scripts/database_scripts_test/poly_dat_files/Humans/BAGOSMOV/EX1/\")\n",
    "dat_path = singleglob(base, \"*.dat\")\n",
    "task_path = singleglob(base, \"*.xls\")\n",
    "info_path = singleglob(base, \"*.yaml\", search_upward_limit=Path(\"/home/julienb/Documents/database_scripts/database_scripts_test/poly_dat_files\"))\n",
    "res_events_path = base/\"events.tsv\"\n",
    "info_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading poly events and adding task information "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_df = pd.read_csv(dat_path, sep=\"\\t\", names=['time (ms)', 'family', 'nbre', '_P', '_V', '_L', '_R', '_T', '_W', '_X', '_Y', '_Z'], skiprows=13, dtype=int)\n",
    "event_df.insert(0, \"t\", event_df.pop(\"time (ms)\")/1000)\n",
    "event_df = event_df.reset_index(names=\"poly_evnum\").sort_values([\"t\", \"poly_evnum\"]).reset_index(drop=True)\n",
    "event_df[\"task_node\"] = event_df[\"_T\"].where(event_df[\"family\"]==10).ffill()\n",
    "event_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_df = pd.read_csv(task_path, sep=\"\\t\", header=11)\n",
    "task_df = task_df.rename(columns={task_df.columns[0]: \"task_node\" })\n",
    "display(task_df.columns)\n",
    "task_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "channels = pd.Series(task_df.columns).str.extract(r'\\s*(?P<channel_name>\\w+)\\s*\\((?P<family>\\d+)\\s*,\\s*(?P<nbre>\\d+)\\)\\s*').assign(taskcol_name=task_df.columns).dropna(how=\"any\")\n",
    "channels[\"family\"] = channels[\"family\"].astype(int)\n",
    "channels[\"nbre\"] = channels[\"nbre\"].astype(int)\n",
    "channels\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern=r'on\\(\\d+(,\\d+)*\\)'\n",
    "task_info=pd.DataFrame()\n",
    "stacked = task_df.set_index(\"task_node\")[channels[\"taskcol_name\"].to_list()].stack().str.lower().str.strip().dropna()\n",
    "stacked.index.names=[\"task_node\", \"taskcol_name\"]\n",
    "task_info[\"data\"] = stacked\n",
    "task_info[\"match\"] = task_info[\"data\"].str.fullmatch(pattern)\n",
    "task_info = task_info.loc[task_info[\"match\"]]\n",
    "task_info[\"important\"] = task_info[\"data\"].str.slice(3, -1)\n",
    "task_info[\"task_params\"] = task_info[\"important\"].str.split(\",\").apply(lambda l: [float(x) for x in l])\n",
    "task_info = task_info.drop(columns=[\"important\", \"match\", \"data\"]).join(stacked.rename(\"task_data\"), how=\"outer\")\n",
    "task_info = task_info.reset_index() \n",
    "task_info[\"task_node\"] = task_info[\"task_node\"].astype(float)\n",
    "task_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_channels_df = channels.merge(event_df, on=[\"family\", \"nbre\"], how=\"right\").merge(task_info, on=[\"taskcol_name\", \"task_node\"], how=\"left\").sort_values(\"t\")\n",
    "event_channels_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting configuration information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = yaml.safe_load(info_path.open(\"r\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_spec = events_methods.FiberEventProcessing.process_info(channels[\"channel_name\"].to_list(), info[\"processing\"])\n",
    "pd.DataFrame(list(event_spec.values()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running event extraction methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_channels_df = event_channels_df.sort_values([\"t\", \"poly_evnum\"])\n",
    "all=[]\n",
    "for ev_name, item in event_spec.items():\n",
    "    ev_dataframe = events_methods.PolyEventProcessing.compute_evdataframe(event_channels_df, item)\n",
    "    if len(ev_dataframe.index) == 0: continue\n",
    "    events = events_methods.PolyEventProcessing.call(item[\"method\"],ev_dataframe, item)\n",
    "    all.append(events.reset_index(drop=True))\n",
    "\n",
    "all = pd.concat(all).sort_values(\"t\")\n",
    "all\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Renaming and exporting, displaying reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"display\" in info and \"rename\" in info[\"display\"]:\n",
    "    all[\"event_name\"] = all[\"event_name\"].map(lambda e: info[\"display\"][\"rename\"][e] if e in info[\"display\"][\"rename\"] else e)\n",
    "json_cols = [\"metadata\", \"waveform_changes\", \"waveform_values\"]\n",
    "for col in json_cols:\n",
    "    all[f\"{col}_json\"] = all[col].apply(lambda d: json.dumps(d))\n",
    "all.drop(columns=json_cols).to_csv(res_events_path, sep=\"\\t\", index=False)\n",
    "reloaded = pd.read_csv(res_events_path, sep=\"\\t\", index_col=False)\n",
    "for col in reloaded.columns:\n",
    "    if col.endswith(\"_json\"):\n",
    "        reloaded[col[:-5]] = reloaded.pop(col).apply(lambda s: json.loads(s) if not pd.isna(s) else None)\n",
    "reloaded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary Information (Checks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = reloaded.copy()\n",
    "\n",
    "def compute_grp(duration):\n",
    "    n = len(duration.index)\n",
    "    rounded = np.round(duration*100)/100\n",
    "    counts = rounded.value_counts()\n",
    "    important = (counts > 2) & (counts > n/5)\n",
    "    important_min_val = counts.loc[important].index.min()\n",
    "    important_max_val = counts.loc[important].index.max()\n",
    "    ret = np.where(rounded.map(important), rounded, \n",
    "          np.where(duration<important_min_val, f\"{rounded.min()}<=v<{important_min_val}\", \n",
    "          np.where(duration>important_max_val, f\"{important_max_val}<v<={rounded.max()}\", \n",
    "          \"_\"\n",
    "        )))\n",
    "    \n",
    "    return ret\n",
    "\n",
    "\n",
    "summary[\"duration_goup\"]= summary.groupby([\"event_name\", \"n_segments\"])[\"duration\"].transform(compute_grp)\n",
    "summary[\"metadata\"].iat[0] = dict(warnings=dict(test=\"2\", other=\"3\"))\n",
    "warning_list = summary[\"metadata\"].apply(lambda d: list(d[\"warnings\"].keys()) if not d is None and \"warnings\"in d else [])\n",
    "all_warnings = set(warning_list.sum())\n",
    "\n",
    "def compute_info(grp):\n",
    "    warnings = {k: 0 for k in all_warnings}\n",
    "    for m in grp[\"metadata\"].values:\n",
    "        if m is None: continue\n",
    "        if \"warnings\" in m:\n",
    "            for k in m[\"warnings\"].keys():\n",
    "                if not k in warnings:\n",
    "                    warnings[k]=0\n",
    "                warnings[k]+=1\n",
    "    return pd.Series(dict(n=len(grp.index)) | {f'warning_{k}': v for k, v in warnings.items()})\n",
    "summary.groupby([\"event_name\", \"n_segments\", \"duration_goup\"]).apply(compute_info, include_groups=False).reset_index()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
