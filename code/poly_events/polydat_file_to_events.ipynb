{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np,xarray as xr\n",
    "from pathlib import Path\n",
    "import re, yaml, copy, json\n",
    "from helper import singleglob, Waveform, json_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = Path(\"/home/julienb/Documents/database_scripts/database_scripts_test/poly_dat_files/Rats/Test_Julien_ForcedInput/\")\n",
    "# base = Path(\"/home/julienb/Documents/database_scripts/database_scripts_test/poly_dat_files/Rats/Luisa/Rat101_0729_opto_01\")\n",
    "\n",
    "dat_path = singleglob(base, \"*.dat\")\n",
    "task_path = singleglob(base, \"*.xls\")\n",
    "info_path = singleglob(base, \"*.yaml\", search_upward_limit=Path(\"/home/julienb/Documents/database_scripts/database_scripts_test/poly_dat_files\"))\n",
    "res_events_path = base/\"events.tsv\"\n",
    "exists = {\"dat_path\":dat_path.exists(), \"task_path\":task_path.exists(), \"info_path\": info_path.exists()}\n",
    "if not np.all(list(exists.values())):\n",
    "    display(exists)\n",
    "    raise Exception(\"Missing some input files...\")\n",
    "info_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_df = pd.read_csv(dat_path, sep=\"\\t\", names=['time (ms)', 'family', 'nbre', '_P', '_V', '_L', '_R', '_T', '_W', '_X', '_Y', '_Z'], skiprows=13, dtype=int)\n",
    "event_df.insert(0, \"t\", event_df.pop(\"time (ms)\")/1000)\n",
    "event_df = event_df.reset_index(names=\"poly_evnum\").sort_values([\"t\", \"poly_evnum\"]).reset_index(drop=True)\n",
    "event_df[\"task_node\"] = event_df[\"_T\"].where(event_df[\"family\"]==10).ffill()\n",
    "print(event_df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_df = pd.read_csv(task_path, sep=\"\\t\", header=11)\n",
    "task_df = task_df.rename(columns={task_df.columns[0]: \"task_node\" })\n",
    "display(task_df.columns)\n",
    "task_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "channels = pd.Series(task_df.columns).str.extract(r'\\s*(?P<channel_name>\\w+)\\s*\\((?P<family>\\d+)\\s*,\\s*(?P<nbre>\\d+)\\)\\s*').assign(taskcol_name=task_df.columns).dropna(how=\"any\")\n",
    "channels[\"family\"] = channels[\"family\"].astype(int)\n",
    "channels[\"nbre\"] = channels[\"nbre\"].astype(int)\n",
    "channels\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern=r'on\\(\\d+(,\\d+)*\\)'\n",
    "task_info=pd.DataFrame()\n",
    "stacked = task_df.set_index(\"task_node\")[channels[\"taskcol_name\"].to_list()].stack().str.lower().str.strip().dropna()\n",
    "stacked.index.names=[\"task_node\", \"taskcol_name\"]\n",
    "task_info[\"data\"] = stacked\n",
    "task_info[\"match\"] = task_info[\"data\"].str.fullmatch(pattern)\n",
    "task_info = task_info.loc[task_info[\"match\"]]\n",
    "task_info[\"important\"] = task_info[\"data\"].str.slice(3, -1)\n",
    "task_info[\"task_params\"] = task_info[\"important\"].str.split(\",\").apply(lambda l: [float(x) for x in l])\n",
    "task_info = task_info.drop(columns=[\"important\", \"match\", \"data\"]).join(stacked.rename(\"task_data\"), how=\"outer\")\n",
    "task_info = task_info.reset_index() \n",
    "task_info[\"task_node\"] = task_info[\"task_node\"].astype(float)\n",
    "task_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_channels_df = channels.merge(event_df, on=[\"family\", \"nbre\"], how=\"right\").merge(task_info, on=[\"taskcol_name\", \"task_node\"], how=\"left\").sort_values(\"t\")\n",
    "print(event_channels_df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = yaml.safe_load(info_path.open(\"r\"))\n",
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_spec = []\n",
    "\n",
    "for item in info[\"processing\"]:\n",
    "    if \"duplicate_over\" not in item:\n",
    "        item[\"duplicate_over\"] = {}\n",
    "    from helper import generate_duplicate_table, replace_vals\n",
    "    duplication = generate_duplicate_table(item[\"duplicate_over\"], dict(channel_name=channels[\"channel_name\"]))\n",
    "    for _, row in duplication.iterrows():\n",
    "        final_d = replace_vals(item, row.to_dict())\n",
    "        ev_name = final_d[\"event_name\"]\n",
    "        if \"display\" in info and \"rename\"in info[\"display\"] and ev_name in info[\"display\"][\"rename\"]:\n",
    "            final_d[\"display_name\"] = info[\"display\"][\"rename\"][ev_name]\n",
    "        else:\n",
    "            final_d[\"display_name\"] = ev_name\n",
    "        del final_d[\"duplicate_over\"]\n",
    "        event_spec.append(final_d)\n",
    "\n",
    "unique_df = pd.DataFrame(event_spec)[\"event_name\"].value_counts().reset_index()\n",
    "if not (unique_df[\"count\"] == 1).all():\n",
    "    display(unique_df.loc[unique_df[\"count\"] > 1])\n",
    "    raise Exception(f\"Event name duplication\")\n",
    "\n",
    "\n",
    "display(pd.DataFrame(event_spec))\n",
    "event_spec = {v[\"event_name\"]: v for v in event_spec}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_channels_df = event_channels_df.sort_values([\"t\", \"poly_evnum\"])\n",
    "event_channels_df[\"used\"]=False\n",
    "from ipywidgets import widgets\n",
    "outs = {k: display(display_id=f'{k}') for k in event_spec}\n",
    "\n",
    "def myeval(df, expr):\n",
    "    task_expr_pattern = re.compile(r'task\\[(?P<col_num>\\d+)\\]')\n",
    "    task_expr_df = pd.DataFrame()\n",
    "    def handle_col(match):\n",
    "        num = int(match[\"col_num\"])\n",
    "        name = f'__task_{num}'\n",
    "        task_expr_df[name] = df[\"task_params\"].apply(lambda l: l[num] if isinstance(l, list) and len(l) > num else np.nan)\n",
    "        return name\n",
    "    new_expr=re.sub(task_expr_pattern, handle_col, expr)\n",
    "    return pd.concat([df, task_expr_df], axis=1).eval(new_expr)\n",
    "\n",
    "def compute_relevant(config):\n",
    "    df = pd.DataFrame()\n",
    "    ev_name = config[\"event_name\"]\n",
    "    df[\"t\"] = event_channels_df[\"t\"]\n",
    "    df[\"task_data\"] = event_channels_df[\"task_data\"]\n",
    "    df[\"task_node\"] = event_channels_df[\"task_node\"]\n",
    "    # task_expr_pattern = re.compile(r'task\\[(?P<col_num>\\d+)\\]')\n",
    "    for param, expr in config[\"method_params\"].items():\n",
    "        if param.endswith(\"_expr\"):\n",
    "            df[param.replace(\"_expr\", \"_value\")] = myeval(event_channels_df, expr)\n",
    "        else:\n",
    "            df[param] = expr\n",
    "    if \"metadata\" in config:\n",
    "        metadata = pd.DataFrame()\n",
    "        for k, v in config[\"metadata\"].items():\n",
    "            if k.endswith(\"_expr\"):\n",
    "                metadata[k.replace(\"_expr\", \"\")] = myeval(event_channels_df, v)\n",
    "            else:\n",
    "                metadata[k] = v\n",
    "        df[\"metadata\"] = metadata.apply(lambda row: {k:v for k, v in row.items() if not pd.isna(v)}, axis=1)\n",
    "    else:\n",
    "        df[\"metadata\"] = [{}] * len(df.index)\n",
    "    filtered = df.loc[df[\"filter_value\"]] if \"filter_expr\" in config[\"method_params\"] else df\n",
    "    if \"state_expr\" in config[\"method_params\"]:\n",
    "        relevant = filtered.loc[filtered[\"state_value\"] != filtered[\"state_value\"].shift(1)].copy()\n",
    "        relevant[\"duration\"] = relevant[\"t\"].shift(-1) - relevant[\"t\"]\n",
    "    else:\n",
    "        relevant = filtered.copy()\n",
    "    outs[config[\"event_name\"]].update(relevant.rename_axis([ev_name]))\n",
    "    return relevant\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_metadata(join_metadata, **warnings):\n",
    "    kept_warnings = {}\n",
    "    for k, v in warnings.items():\n",
    "        if isinstance(v, dict):\n",
    "            vals = {v for v in v.values() if not pd.isna(v)}\n",
    "            if len(vals) > 1:\n",
    "                kept_warnings[k] = v\n",
    "        else:\n",
    "            if v[0]:\n",
    "                kept_warnings[k] = v[1]\n",
    "    if len(kept_warnings) > 0:\n",
    "        metadata = json_merge(*join_metadata, dict(warnings=kept_warnings))\n",
    "    else:\n",
    "        metadata = json_merge(*join_metadata)\n",
    "    return metadata\n",
    "\n",
    "\n",
    "def output_accumulator_binary_wave(relevant: pd.DataFrame, config):\n",
    "    res = []\n",
    "    for _,grp in  relevant.groupby((relevant[\"state_value\"] ==1).cumsum()):\n",
    "        starts = grp[grp[\"state_value\"]==1]\n",
    "        if len(starts.index) ==0: continue\n",
    "        elif len(starts.index) > 1: display(grp); raise Exception(f\"Problem {len(starts.index)}\")\n",
    "        else:\n",
    "            start = starts[\"t\"].iat[0]\n",
    "            rises = [0] + (grp[\"t\"].loc[grp[\"state_value\"] > grp[\"state_value\"].shift(1)] -start).to_list()\n",
    "            duration_on = grp[\"duration_on_value\"].iat[0]\n",
    "            expected_count = grp[\"expected_count_value\"].iat[0]\n",
    "            duration = rises[-1] + duration_on\n",
    "            metadata_join = grp[\"metadata\"].to_list()\n",
    "            if pd.isnull(grp[\"task_data\"].iat[0]) or grp[\"task_data\"].iat[0].startswith(\"on\"):\n",
    "                metadata_join.append(dict(warnings=dict(free_event=f'task_data_at_event_is {grp[\"task_data\"].iat[0]}')))\n",
    "            metadata = compute_metadata(grp[\"metadata\"], \n",
    "                count_mismatch=dict(read=len(rises), expected=expected_count),\n",
    "                free_event=(pd.isnull(grp[\"task_data\"].iat[0]) or not grp[\"task_data\"].iat[0].startswith(\"on\"), f'task_data_at_event_is {grp[\"task_data\"].iat[0]}')\n",
    "            )\n",
    "            res.append(dict(t=start, metadata=metadata, duration=duration, waveform_info=\n",
    "                dict(type=\"binary\", rises=rises, durations=[duration_on]*len(rises))))\n",
    "    return pd.DataFrame(res)\n",
    "\n",
    "def input_binary_wave(relevant: pd.DataFrame, config):\n",
    "    if relevant[\"state_value\"].iat[0] == 0:\n",
    "        relevant = relevant.iloc[1:, :].copy()\n",
    "    relevant[\"next_metadata\"] = relevant[\"metadata\"].shift(-1)\n",
    "    rises = relevant.iloc[::2].copy()\n",
    "    rises[\"metadata\"] = rises.apply(lambda row: compute_metadata([row[\"metadata\"], row[\"next_metadata\"]]) , axis=1)\n",
    "    return rises[[\"t\", \"duration\", \"metadata\"]].assign(waveform_info=rises[\"duration\"].apply(lambda d: \n",
    "        dict(type=\"binary\", rises=[0], durations=[d])))\n",
    "    \n",
    "\n",
    "def output_binary_wave(relevant: pd.DataFrame, config):\n",
    "    if relevant[\"state_value\"].iat[0] == 0:\n",
    "        relevant = relevant.iloc[1:, :].copy()\n",
    "    else:\n",
    "        relevant= relevant.copy()\n",
    "    relevant[\"next_metadata\"] = relevant[\"metadata\"].shift(-1)\n",
    "    relevant[\"count_value\"] = relevant[\"count_value\"].shift(-1).replace(0, 1)\n",
    "    rises = relevant.iloc[::2].copy()\n",
    "\n",
    "    rises[\"read_duration\"] = rises[\"duration\"]\n",
    "    rises[\"duration\"] = np.where(\n",
    "        (rises[\"duration\"] <= 0.001) & rises[\"duration_on_value\"].notna(), \n",
    "        rises[\"duration_on_value\"], rises[\"duration\"])\n",
    "    rises[\"duration_on_value\"] = rises[\"duration_on_value\"].fillna(rises[\"duration\"])\n",
    "    rises[\"cycle_duration\"] = rises[\"duration_on_value\"] + rises[\"duration_off_value\"]\n",
    "    rises[\"rises\"] = rises.apply(lambda row: [i * row[\"cycle_duration\"] for i in range(int(row[\"count_value\"]))] if pd.notna(row[\"count_value\"]) else np.nan, axis=1)\n",
    "\n",
    "    rises[\"metadata\"] = rises.apply(lambda row: compute_metadata(\n",
    "        [row[\"metadata\"], row[\"next_metadata\"]], \n",
    "        count_mismatch=dict(read=row[\"count_value\"], expected=row[\"expected_count_value\"]),\n",
    "        duration_correction=dict(read=row[\"read_duration\"], corrected=row[\"duration\"]),\n",
    "        free_event=(pd.isnull(row[\"task_data\"]) or not row[\"task_data\"].startswith(\"on\"), f'task_data_at_event_is {row[\"task_data\"]}')\n",
    "    ), axis=1)\n",
    "    \n",
    "    rises[\"waveform_info\"] = rises.apply(lambda row: \n",
    "        dict(type=\"binary\", rises=row[\"rises\"], durations=[row[\"duration_on_value\"]]* len(row[\"rises\"])), axis=1)\n",
    "    return rises[[\"t\",\"metadata\", \"duration\", \"waveform_info\"]].copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "all = []\n",
    "for ev_name, item in event_spec.items():\n",
    "    relevant = compute_relevant(item)\n",
    "    if len(relevant.index) ==0:\n",
    "        continue\n",
    "    if item[\"method\"] == \"output_accumulator_binary_wave\":\n",
    "        events = output_accumulator_binary_wave(relevant, item)\n",
    "    elif item[\"method\"] == \"input_binary_wave\":\n",
    "        events = input_binary_wave(relevant, item)\n",
    "    elif item[\"method\"] == \"output_binary_wave\":\n",
    "        events = output_binary_wave(relevant, item)\n",
    "    elif item[\"method\"]==\"step_wave\":\n",
    "        events = relevant.assign(curr_value= relevant[\"state_value\"], prev_value=relevant[\"state_value\"].shift(1), next_value=relevant[\"state_value\"].shift(-1))\n",
    "        events[\"waveform_info\"] = events[[\"curr_value\", \"prev_value\", \"next_value\"]].apply(lambda row: row.to_dict(), axis=1)\n",
    "        events = events[[\"t\",\"metadata\", \"duration\", \"waveform_info\"]]\n",
    "    elif item[\"method\"]==\"event\":\n",
    "        events = relevant[[\"t\", \"metadata\"]].assign(duration=np.nan, waveform_info=None)\n",
    "    if len(events.index)!=0:\n",
    "        all.append(events[[c for c in events.columns if events[c].count() > 0]].assign(event_name=ev_name))\n",
    "all = pd.concat(all).sort_values(\"t\")\n",
    "display(all)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"display\" in info and \"rename\" in info[\"display\"]:\n",
    "    all[\"event_name\"] = all[\"event_name\"].map(lambda e: info[\"display\"][\"rename\"][e] if e in info[\"display\"][\"rename\"] else e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all[\"metadata_json\"] = all[\"metadata\"].apply(lambda d: json.dumps(d) if not pd.isna(d) else \"{}\")\n",
    "all[\"waveform_info_json\"] = all[\"waveform_info\"].apply(lambda d: json.dumps(d) if not pd.isna(d) else \"{}\")\n",
    "\n",
    "all.drop(columns=[\"waveform_info\", \"metadata\"]).to_csv(res_events_path, sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reloaded = pd.read_csv(res_events_path, sep=\"\\t\", index_col=False)\n",
    "reloaded[\"metadata\"] = reloaded[\"metadata_json\"].apply(lambda s: json.loads(s))\n",
    "reloaded[\"waveform_info\"] = reloaded[\"waveform_info_json\"].apply(lambda s: json.loads(s))\n",
    "reloaded.drop(columns=[\"waveform_info_json\", \"metadata_json\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
